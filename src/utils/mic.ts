// src/utils/mic.ts

/** ë§ˆì´í¬ ê¶Œí•œ: ì„±ê³µ true / ì‹¤íŒ¨ false */
export async function ensureMicPermission(): Promise<boolean> {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          noiseSuppression: true,
          echoCancellation: true,
          autoGainControl: true,
          channelCount: 1,
          sampleRate: 16000
        }
      });
      stream.getTracks().forEach(t => t.stop());
      return true;
    } catch (err) {
      console.error("ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ì‹¤íŒ¨:", err);
      return false;
    }
  }
  
  /** ì•ˆì „ ì—ëŸ¬ í˜¸ì¶œ(ì½œë°± ì—†ìœ¼ë©´ ì½˜ì†”ë§Œ) */
  function safeError(onError: ((m: string) => void) | undefined, msg: string) {
    console.error(msg);
    if (typeof onError === "function") { try { onError(msg); } catch (e) { console.error(e); } }
  }
  
  // >>> @MIC_LISTEN_ONCE_P_SIMPLE
// === ê²¹ì ìœ  ì°¨ë‹¨ìš© ì „ì—­ ===
let __activeStream: MediaStream | null = null;
let __activeAudioCtx: AudioContext | null = null;
let __activeRecognition: any | null = null;

export function forceKillMic() {
  try { __activeRecognition?.stop?.(); } catch {}
  __activeRecognition = null;
  try { __activeStream?.getTracks?.().forEach(t => t.stop()); } catch {}
  __activeStream = null;
  try { 
    if (__activeAudioCtx && __activeAudioCtx.state !== 'closed') {
      __activeAudioCtx.close(); 
    }
  } catch {}
  __activeAudioCtx = null;
}

/**
 * ìŒì„±ì„ í•œ ë²ˆë§Œ ë“£ê³  í…ìŠ¤íŠ¸ ë°˜í™˜ (ê°„ê²°íŒ)
 * - ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ êµ¬ì¡°
 * - ìµœì¢… ê²°ê³¼ ìš°ì„ , interim ë°±ì—…
 * - ê²¹ì ìœ  ë°©ì§€ ì—†ì´ ìˆœìˆ˜ STTë§Œ
 */
export function listenOnceP(locale="ko-KR", totalTimeout=20000, preSpeechTimeout=8000, resultTimeout=10000){
  return new Promise<string>((resolve, reject) => {
    const SR:any = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    if (!SR) return reject(new Error("ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."));

    const rec:any = new SR();
    rec.lang = locale;
    rec.interimResults = true;         // ğŸ”¸ ì¤‘ê°„ ê²°ê³¼ë„ ë°›ê¸°
    rec.maxAlternatives = 3;
    rec.continuous = false;

    let finished = false, gotSpeech = false;
    let bestFinal = "", bestInterim = "";

    const finish = (fn:()=>void) => { if (finished) return; finished = true;
      try { rec.onresult = rec.onerror = rec.onend = rec.onaudiostart = rec.onsoundstart = rec.onspeechstart = null; } catch {}
      try { rec.stop(); } catch {}
      fn();
    };

    const tTotal = setTimeout(() => finish(() => reject(new Error("ì‹œê°„ ì´ˆê³¼: ì¸ì‹ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."))), totalTimeout);
    let tPre:any, tRes:any;
    const clearTimers = ()=>{ clearTimeout(tTotal); clearTimeout(tPre); clearTimeout(tRes); };

    const armRes = ()=>{ clearTimeout(tRes); tRes=setTimeout(()=>{
      if (!finished) {
        const text = bestFinal || bestInterim || "";
        if (text) finish(()=>{ clearTimers(); resolve(text); });
        else finish(()=>{ clearTimers(); reject(new Error("ì¸ì‹ì´ ì¡°ê¸°ì— ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")); });
      }
    }, resultTimeout); };

    const mark = ()=>{ if (!gotSpeech){ gotSpeech = true; armRes(); } };
    rec.onaudiostart = mark; rec.onsoundstart = mark; rec.onspeechstart = mark;

    tPre = setTimeout(()=>{ if (!gotSpeech) finish(()=>{ clearTimers(); reject(new Error("ë§ì†Œë¦¬ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")); }); }, preSpeechTimeout);

    rec.onresult = (e:any) => {
      try {
        console.log("ğŸ” onresult ì´ë²¤íŠ¸ ë°œìƒ!");
        console.log("ğŸ“Š results:", e.results);
        
        for (let i=e.resultIndex; i<e.results.length; i++){
          const r=e.results[i], a=r?.[0];
          const t=(a?.transcript||"").trim();
          if (!t) continue;
          if (r.isFinal) bestFinal = t; else bestInterim = t;
        }
        if (gotSpeech) armRes();
        if (bestFinal) finish(()=>{ clearTimers(); resolve(bestFinal); }); // ìµœì¢… ëœ¨ë©´ ì¦‰ì‹œ ë°˜í™˜
      } catch {}
    };

    rec.onerror = (e:any) => { finish(()=>{ clearTimers(); reject(new Error(e?.error || "ìŒì„± ì¸ì‹ ì˜¤ë¥˜")); }); };

    rec.onend = () => {
      console.log("ğŸ”š onend ì´ë²¤íŠ¸ ë°œìƒ");
      if (!finished) {
        const text = bestFinal || bestInterim || "";
        if (text) {
          console.log("âœ… onendì—ì„œ í…ìŠ¤íŠ¸ í™•ì¸:", text);
          finish(()=>{ clearTimers(); resolve(text); });
        } else {
          console.log("âš ï¸ onendì—ì„œ í…ìŠ¤íŠ¸ ì—†ìŒ");
          finish(()=>{ clearTimers(); reject(new Error("ì¸ì‹ì´ ì¡°ê¸°ì— ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")); });
        }
      }
    };

    try { 
      rec.start(); 
      console.log("ğŸ¤ SpeechRecognition ì‹œì‘ë¨");
    } catch { 
      finish(()=> reject(new Error("SpeechRecognition ì‹œì‘ ì‹¤íŒ¨"))); 
    }
  });
}
// <<< @MIC_LISTEN_ONCE_P_SIMPLE
  
  /** (ì˜µì…˜) ì—°ì† ì¸ì‹ì´ í•„ìš”í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í—¬í¼ */
  export function startListening(
    locale: string,
    onResult: (text: string) => void,
    onError?: (message: string) => void
  ) {
    try {
      const SR: any = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
      if (!SR) { safeError(onError, "ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Chromeì„ ì‚¬ìš©í•´ ì£¼ì„¸ìš”."); return () => {}; }
  
      const rec: any = new SR();
      rec.lang = locale || "ko-KR";
      rec.interimResults = true;
      rec.continuous = true;
  
      rec.onresult = (e: any) => {
        let acc = "";
        for (let i = e.resultIndex; i < e.results.length; i++) {
          acc += e.results[i][0].transcript;
        }
        onResult(acc.trim());
      };
  
      rec.onerror = (e: any) => {
        const code = e?.error;
        let msg = "ìŒì„± ì¸ì‹ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.";
        if (code === "not-allowed") msg = "ë§ˆì´í¬ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ì£¼ì†Œì°½ ì™¼ìª½ ğŸ”’ì—ì„œ 'í—ˆìš©'ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”.";
        if (code === "audio-capture") msg = "ë§ˆì´í¬ ì¥ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OSì˜ ì…ë ¥ ì¥ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.";
        if (code === "no-speech") msg = "ë§ì†Œë¦¬ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¡°ê¸ˆ ë” í¬ê²Œ ë˜ë ·í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”.";
        safeError(onError, msg);
      };
  
      rec.start();
      return () => { try { rec.stop(); } catch {} };
    } catch {
      safeError(onError, "startListening ì´ˆê¸°í™” ì‹¤íŒ¨");
      return () => {};
    }
  }
  
// >>> @MIC_VU_METER
/**
 * ì‹¤ì‹œê°„ ë§ˆì´í¬ ë ˆë²¨ ë¯¸í„°
 * - onLevel: 0~1 ì‚¬ì´ ì…ë ¥ ê°•ë„ ì½œë°±(ì´ˆë‹¹ 30~60íšŒ)
 * - ë°˜í™˜: stop() í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¼/ì˜¤ë””ì˜¤ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬)
 */
export async function startMicLevelMeter(onLevel: (v: number) => void) {
  // ğŸ”´ ë‹¤ë¥¸ ì ìœ  ëª¨ë‘ ì¢…ë£Œ
  forceKillMic();

  const stream = await navigator.mediaDevices.getUserMedia({
    audio: {
      noiseSuppression: true,
      echoCancellation: true,
      autoGainControl: true,
      channelCount: 1,
      sampleRate: 16000
    }
  });
  __activeStream = stream;

  const Ctx: any = (window as any).AudioContext || (window as any).webkitAudioContext;
  const ctx = new Ctx();
  __activeAudioCtx = ctx;

  const src = ctx.createMediaStreamSource(stream);
  const analyser = ctx.createAnalyser();
  analyser.fftSize = 1024;
  const data = new Uint8Array(analyser.frequencyBinCount);
  src.connect(analyser);

  let raf = 0;
  const loop = () => {
    analyser.getByteTimeDomainData(data);
    // íŒŒí˜•ì˜ ì¤‘ì•™(128)ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚˜ëŠ”ì§€ë¡œ ë ˆë²¨ ê³„ì‚°
    let peak = 0;
    for (let i = 0; i < data.length; i++) {
      const dev = Math.abs(data[i] - 128) / 128; // 0~1
      if (dev > peak) peak = dev;
    }
    onLevel(Math.min(1, peak * 2)); // ë¯¼ê°ë„ ì‚´ì§ ì˜¬ë¦¼
    raf = requestAnimationFrame(loop);
  };
  loop();

  const stop = () => {
    cancelAnimationFrame(raf);
    try { src.disconnect(); } catch {}
    try { analyser.disconnect(); } catch {}
    try { stream.getTracks().forEach(t => t.stop()); } catch {}
    try { ctx.close(); } catch {}
  };
  return stop;
}
// <<< @MIC_VU_METER

// === ë””ë²„ê¹…ìš© ì „ì—­ ë…¸ì¶œ ===
(window as any).debugListenOnceP = listenOnceP;
(window as any).debugForceKillMic = forceKillMic;
  